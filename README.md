# DataOps Pipeline - Bronze, Silver, Gold

Pipeline completo de dados com arquitetura em camadas, orquestra√ß√£o Airflow e dashboard em tempo real.

## Arquitetura

```
Label Studio (NER Annotations)
         ‚Üì
    Bronze Layer (Raw JSON)
         ‚Üì
    Silver Layer (Clean + NER Extraction)
         ‚Üì
    Gold Layer (Aggregations + KPIs)
         ‚Üì
    Streamlit Dashboard
```

## Stack Tecnol√≥gica

- **Orquestra√ß√£o**: Apache Airflow (event-driven com sensores deferr√°veis)
- **Storage**: MinIO (S3-compatible, 3 camadas)
- **Transforma√ß√£o**: Python + Pandas
- **Anota√ß√£o**: Label Studio (NER)
- **Visualiza√ß√£o**: Streamlit
- **Infraestrutura**: Docker + Docker Compose
- **Banco de Dados**: PostgreSQL

## Quick Start (Usando Makefile)

> **Primeira vez usando?** Veja [INSTALACAO_AMBIENTE.md](INSTALACAO_AMBIENTE.md) para instala√ß√£o completa do zero (Conda + UV + Docker)

### Op√ß√£o 1: Com Makefile (Recomendado) ‚ö°

**Setup em 2 comandos:**

```bash
# 1. Clone o reposit√≥rio
git clone <URL>
cd Dataops

# 2. Suba a infraestrutura (configura .env automaticamente)
make up
```

O comando `make up` ir√°:
- ‚úÖ Criar arquivo `.env` com credenciais padr√£o (se n√£o existir)
- ‚úÖ Subir todos os containers Docker
- ‚úÖ Configurar MinIO, Airflow, Label Studio e Dashboard

**Acesse os servi√ßos:**
- **Airflow**: http://localhost:8080 (airflow/airflow)
- **Label Studio**: http://localhost:8001 (admin@localhost.com/123456)
- **MinIO**: http://localhost:9001 (dataops_admin/DataOps2025!SecurePassword)
- **Dashboard**: http://localhost:8501

```bash
# 3. Configure Label Studio token (veja se√ß√£o abaixo)
# Edite .env e adicione seu LABELSTUDIO_TOKEN

# 4. Execute o pipeline
make run

# 5. Visualize o dashboard
make dashboard
```

**Ambiente Python (opcional - apenas para desenvolvimento local):**
```bash
make setup
conda activate dataops
```

### Op√ß√£o 2: Comandos Manuais

```bash
# 1. Clone o reposit√≥rio
git clone <URL>
cd Dataops

# 2. Configure o ambiente Python (opcional - local)
conda create -n dataops python=3.10 -y
conda activate dataops
uv sync --directory environments

# 3. Configure as credenciais (OPCIONAL - j√° sobe com padr√£o)
cp .env.example .env
# Edite .env apenas se quiser alterar credenciais

# 4. Inicie o ambiente Docker
docker-compose up -d

# 5. Acesse os servi√ßos (mesmas URLs acima)
```

### Comandos do Makefile Dispon√≠veis

```bash
make          # Ver todos os comandos dispon√≠veis
make setup    # Criar ambiente conda + instalar depend√™ncias
make install  # Instalar/atualizar depend√™ncias
make up       # Subir containers Docker
make down     # Parar containers
make restart  # Reiniciar containers
make logs     # Ver logs dos containers
make clean    # Limpar buckets MinIO
make run      # Executar pipeline completo
make dashboard # Abrir dashboard Streamlit
make test     # Rodar testes
make lint     # Verificar c√≥digo
```

## Documenta√ß√£o Completa

### Para Come√ßar
- **[INSTALACAO_AMBIENTE.md](INSTALACAO_AMBIENTE.md)** - Instala√ß√£o completa do ambiente do zero
  - Instalar Conda/Miniconda
  - Criar ambiente Python 3.10
  - Instalar UV (gerenciador de depend√™ncias)
  - Instalar Docker + Docker Compose
  - Configurar e executar todo o pipeline

- **[SETUP_COMPLETO.md](SETUP_COMPLETO.md)** - Guia r√°pido (se j√° tem ambiente)
  - Pr√©-requisitos
  - Instala√ß√£o
  - Configura√ß√£o do Legacy Token do Label Studio
  - Execu√ß√£o do pipeline
  - Troubleshooting

## IMPORTANTE: Label Studio Legacy Token e Project ID

Este pipeline requer o **Legacy Token** do Label Studio, n√£o o Access Token (JWT).

**Tutorial em v√≠deo**:
- Como criar o Legacy Token: https://drive.google.com/file/d/11teN7OjPgbhWD17H0z4XPJ5pYhE3D4_j/view?usp=sharing
- Como criar o projeto: https://drive.google.com/file/d/1sC-S7fQ0PFElqM8oX01OP-f2IsGlSrx_/view?usp=sharing

**Como configurar e obter**:

1. **Acesse Label Studio**: http://localhost:8001
2. **Primeiro acesso**: Crie conta com `label_ops@gmail.com` / `dataops@123`
   - Ou use credenciais padr√£o: `admin@localhost.com` / `123456`

3. **Habilitar Legacy Tokens (evitar expira√ß√£o)**:
   - Clique em **"Organization"** (menu lateral)
   - Clique em **"API Tokens Settings"**
   - Deixe APENAS a flag **"Legacy tokens"** marcada
   - Desmarque as outras op√ß√µes
   - Clique em **"Save"**

4. **Copiar o token**:
   - Clique no √≠cone do usu√°rio (canto superior direito)
   - **Account & Settings**
   - Procure por **"Legacy API Token"**
   - Copie o token (40 caracteres hexadecimais)

5. **Obter ID do Projeto**:
   - Acesse o projeto no Label Studio
   - Veja a URL do navegador: `http://localhost:8001/projects/3/data?tab=3`
   - O n√∫mero ap√≥s `/projects/` √© o ID do projeto (neste exemplo: `3`)

6. **Inserir no arquivo .env**:
   ```env
   LABELSTUDIO_TOKEN=seu_token_aqui_40_caracteres
   LABELSTUDIO_PROJECT=3  # ID do seu projeto
   ```

Ver detalhes completos em **[SETUP_COMPLETO.md](SETUP_COMPLETO.md) - Passo 4.3**

## Dataset do Projeto

O projeto utiliza um dataset de transa√ß√µes comerciais com anota√ß√µes NER.

**Baixar dataset**:
```
https://drive.google.com/drive/folders/1WFkw54HojR1y_Io26_cNV5ni3888I2FZ?usp=sharing
```

**Conte√∫do**:
- **1000 registros totais** de transa√ß√µes comerciais
  - 500 registros completos e v√°lidos (cliente, produto, valor, etc.)
  - 500 registros incompletos ou com falhas (para testar valida√ß√£o)
- Anota√ß√µes NER j√° realizadas
- Pronto para importa√ß√£o no Label Studio
- **Arquivo de teste event-driven**: Inclui arquivo JSON para testar detec√ß√£o autom√°tica de novos arquivos no bucket Bronze (inbox)

**Prop√≥sito do dataset misto**:
- Testar a **camada Silver** com valida√ß√µes de qualidade de dados
- Demonstrar **pipeline robusto** que identifica e remove dados inv√°lidos
- Calcular **m√©tricas de qualidade** (taxa de limpeza, reten√ß√£o)
- **Testar event-driven**: Arquivo adicional para simular chegada de novos dados no inbox

**Como usar**:
1. Baixe o dataset do Google Drive
2. Importe no Label Studio (Project ID 4)
3. Execute o pipeline via Airflow
4. Visualize resultados no Dashboard

**Resultado esperado ap√≥s processamento**:
- Bronze: 1000 registros (dados brutos)
- Silver: ~500 registros (ap√≥s valida√ß√£o)
- Gold: ~500 registros (agregados)
- Taxa de limpeza: ~50%

Ver instru√ß√µes completas em **[INSTALACAO_AMBIENTE.md](INSTALACAO_AMBIENTE.md) - Parte 8.6**

## Principais Features

- **Event-Driven Architecture** - Airflow detecta novos arquivos automaticamente
- **3 Camadas (Bronze/Silver/Gold)** - Arquitetura Medallion completa
- **Valida√ß√£o de Qualidade de Dados** - Pipeline remove ~50% de registros inv√°lidos
- **NER Extraction** - Named Entity Recognition via Label Studio
- **Monitoramento de Pipeline** - Dashboard com m√©tricas Bronze/Silver/Gold
- **Debug Autom√°tico** - Logs detalhados de extra√ß√£o e valida√ß√£o
- **Seguran√ßa** - Credenciais em vari√°veis de ambiente (zero hardcoded)
- **Auto-detec√ß√£o de Ambiente** - Funciona local e Docker sem mudan√ßas
- **Dashboard Real-time** - Streamlit com atualiza√ß√£o autom√°tica (TTL 60s)

## M√©tricas

- **1000 registros** no dataset (500 v√°lidos + 500 inv√°lidos)
- **~50% taxa de limpeza** (valida√ß√£o Silver remove registros problem√°ticos)
- **8 KPIs** pr√©-calculados na camada Gold
- **3 camadas** de storage (Bronze/Silver/Gold)
- **0 credenciais** hardcoded (100% vari√°veis de ambiente)
- **100% containerizado** (Docker + Docker Compose)

## Desenvolvimento

### Automa√ß√£o com Makefile

O projeto inclui um **Makefile** para automatizar tarefas comuns:

```bash
# Ver todos os comandos dispon√≠veis
make help

# Setup completo (primeira vez)
make first-run

# Executar pipeline completo
make pipeline

# Gerenciar Docker
make docker-up       # Subir containers
make docker-down     # Parar containers
make docker-logs     # Ver logs

# Dashboard local
make dashboard

# Limpeza
make clean-buckets
```

### Executar localmente (fora do Docker)

```bash
# Instalar depend√™ncias
pip install -r requirements.txt

# Executar pipeline
python -m scripts_pipeline.clean_buckets
python -m scripts_pipeline.insert_bronze
python -m scripts_pipeline.transform_silver
python -m scripts_pipeline.aggregate_gold

# Ver diagn√≥stico
python diagnose_data_flow.py

# Visualizar dashboard (ap√≥s dados subirem para camada Gold)
streamlit run streamlit\dashboard.py
```

> **NOTA**: Se estiver usando Docker, o dashboard j√° est√° rodando automaticamente em http://localhost:8501

### Estrutura de Diret√≥rios

```
Dataops/
‚îú‚îÄ‚îÄ dags/                    # Airflow DAGs
‚îÇ   ‚îú‚îÄ‚îÄ sensors/            # Sensores customizados
‚îÇ   ‚îî‚îÄ‚îÄ env_config.py       # Configura√ß√£o segura
‚îú‚îÄ‚îÄ scripts_pipeline/        # Scripts de transforma√ß√£o
‚îÇ   ‚îú‚îÄ‚îÄ insert_bronze.py    # Ingest√£o
‚îÇ   ‚îú‚îÄ‚îÄ transform_silver.py # Limpeza + NER
‚îÇ   ‚îî‚îÄ‚îÄ aggregate_gold.py   # Agrega√ß√µes
‚îú‚îÄ‚îÄ streamlit/              # Dashboard
‚îÇ   ‚îî‚îÄ‚îÄ dashboard.py
‚îú‚îÄ‚îÄ docker-compose.yml      # Orquestra√ß√£o
‚îú‚îÄ‚îÄ .env.example           # Template de configura√ß√£o
‚îî‚îÄ‚îÄ docs/                   # Documenta√ß√£o completa
```

## Troubleshooting

### Container n√£o inicia
```bash
docker-compose logs <container_name>
```

### Erro "Failed to resolve 'minio'"
J√° corrigido! O sistema detecta automaticamente o ambiente.

### Label Studio - 401 Unauthorized
Certifique que est√° usando **Legacy Token**, n√£o Access Token.

Ver mais em **[SETUP_COMPLETO.md](SETUP_COMPLETO.md) - Troubleshooting**

## Fluxo de Dados

1. **Ingest√£o**: Label Studio API ‚Üí JSON estruturado ‚Üí MinIO Bronze
2. **Transforma√ß√£o**: Bronze ‚Üí Limpeza + NER extraction ‚Üí MinIO Silver
3. **Agrega√ß√£o**: Silver ‚Üí KPIs + Agrega√ß√µes ‚Üí MinIO Gold
4. **Visualiza√ß√£o**: Gold ‚Üí Streamlit Dashboard

## Conceitos Aplicados

- **DataOps**: Orquestra√ß√£o, monitoramento, versionamento
- **Medallion Architecture**: Bronze (raw) ‚Üí Silver (clean) ‚Üí Gold (curated)
- **Event-Driven**: Processamento reativo a eventos
- **NER (Named Entity Recognition)**: Extra√ß√£o de entidades nomeadas
- **Containeriza√ß√£o**: Docker, isolamento, portabilidade
- **Security**: Credenciais em vari√°veis de ambiente

## Decis√µes Arquiteturais e Limita√ß√µes

### Decis√µes de Design

#### 1. Arquitetura Medallion (Bronze ‚Üí Silver ‚Üí Gold)
**Por qu√™?**
- **Separa√ß√£o de responsabilidades**: Cada camada tem prop√≥sito claro (raw, clean, curated)
- **Rastreabilidade**: Dados brutos preservados em Bronze para auditoria
- **Reprocessamento**: Possibilidade de reprocessar apenas camadas espec√≠ficas
- **Evolu√ß√£o gradual**: Transforma√ß√µes incrementais facilitam debugging

**Implementa√ß√£o**:
- Bronze: Dados brutos em JSON do Label Studio
- Silver: Dados limpos em Parquet com valida√ß√µes e NER extra√≠do
- Gold: Agrega√ß√µes pr√©-calculadas para dashboard

#### 2. Event-Driven com Sensores Deferr√°veis
**Por qu√™?**
- **Efici√™ncia de recursos**: Sensores deferr√°veis liberam workers enquanto aguardam eventos
- **Processamento reativo**: Pipeline processa automaticamente quando novos dados chegam
- **Escalabilidade**: N√£o desperdi√ßa recursos esperando ativamente por arquivos

**Implementa√ß√£o**:
- `S3KeySensor` em modo deferrable monitora bucket Bronze
- DAG acionada automaticamente quando novo arquivo JSON aparece
- Triggerer do Airflow gerencia sensores de forma ass√≠ncrona

#### 3. MinIO ao inv√©s de S3 Real
**Por qu√™?**
- **Ambiente local**: Desenvolvimento e testes sem custos de cloud
- **Compatibilidade S3**: API 100% compat√≠vel, facilita migra√ß√£o futura
- **Controle total**: Dados permanecem localmente durante desenvolvimento

#### 4. Label Studio para Anota√ß√µes NER
**Por qu√™?**
- **Interface visual**: Facilita anota√ß√£o de entidades sem c√≥digo
- **API robusta**: Integra√ß√£o program√°tica para extra√ß√£o de dados
- **Open source**: Sem custos de licenciamento

#### 5. Streamlit para Dashboard
**Por qu√™?**
- **Rapidez de desenvolvimento**: Dashboard funcional em poucas linhas Python
- **Integra√ß√£o nativa**: Trabalha nativamente com Pandas e Plotly
- **Atualiza√ß√£o em tempo real**: Cache TTL de 60s para dados sempre atualizados

### Limita√ß√µes Conhecidas

#### 1. Escalabilidade de Volume
- **Limita√ß√£o**: Pipeline projetado para ~500 registros, n√£o testado em milh√µes
- **Impacto**: Pandas pode ter problemas de mem√≥ria com datasets muito grandes
- **Mitiga√ß√£o futura**: Migrar para PySpark ou Dask para processamento distribu√≠do

#### 2. Processamento S√≠ncrono
- **Limita√ß√£o**: Transforma√ß√µes executam sequencialmente (Bronze ‚Üí Silver ‚Üí Gold)
- **Impacto**: N√£o aproveita paraleliza√ß√£o para m√∫ltiplos arquivos
- **Mitiga√ß√£o futura**: Implementar processamento paralelo com Celery ou Ray

#### 3. Valida√ß√£o de Dados
- **Implementa√ß√£o**: Valida√ß√µes de campos obrigat√≥rios, tipos, dados n√£o vazios
- **Dataset misto intencional**: 1000 registros (500 v√°lidos + 500 inv√°lidos)
  - Demonstra robustez do pipeline em lidar com dados problem√°ticos
  - Calcula m√©tricas de qualidade (taxa de limpeza ~50%)
- **Limita√ß√£o**: Valida√ß√µes simples, sem regras de neg√≥cio complexas
- **Mitiga√ß√£o futura**: Integrar Great Expectations para valida√ß√µes avan√ßadas

#### 4. Aus√™ncia de Versionamento de Schema
- **Limita√ß√£o**: Mudan√ßas no schema Label Studio podem quebrar pipeline
- **Impacto**: Necessidade de ajustes manuais ao evoluir schema
- **Mitiga√ß√£o futura**: Implementar schema registry (Apache Avro/Protobuf)

#### 5. Monitoramento e Alertas Limitados
- **Limita√ß√£o**: Logs b√°sicos do Airflow, sem alertas proativos
- **Impacto**: Falhas podem passar despercebidas
- **Mitiga√ß√£o futura**: Integrar Prometheus + Grafana + alertas por email/Slack

## Valida√ß√£o de Resultados

### Como Validar Cada Camada do Pipeline

#### 1. Camada Bronze (Raw Data)

**Onde verificar**:
- **MinIO Console**: http://localhost:9001
  - Login com credenciais do `.env`
  - Navegue para bucket `bronze`
  - Deve conter arquivos `bronze_YYYYMMDD_HHMMSS.json`

**O que esperar**:
```json
[
  {
    "id": 1,
    "data": {"text": "Cliente Jo√£o comprou notebook por R$ 2500"},
    "annotations": [...]
  }
]
```

**Valida√ß√£o via CLI**:
```bash
# Listar arquivos Bronze
python -c "from minio import Minio; from env_config import get_minio_config; cfg = get_minio_config(); client = Minio(endpoint=cfg['endpoint'], access_key=cfg['access_key'], secret_key=cfg['secret_key'], secure=False); print(list(client.list_objects('bronze')))"
```

#### 2. Camada Silver (Clean Data + NER)

**Onde verificar**:
- **MinIO Console**: http://localhost:9001
  - Bucket `silver`
  - Arquivos `silver_YYYYMMDD_HHMMSS.parquet`

**O que esperar**:
- Dados em formato Parquet
- Colunas NER extra√≠das: `cliente_ner`, `produto_ner`, `valor_ner`, etc.
- Registros inv√°lidos removidos (valida√ß√£o de ID, data n√£o vazia)

**Logs de valida√ß√£o**:
```bash
# Ver logs do Airflow para transform_silver
docker-compose logs airflow-scheduler | grep "transform_silver"

# Ou acessar Airflow UI
# http://localhost:8080 ‚Üí DAGs ‚Üí 00_event_driven_ingestion ‚Üí Task transform_silver ‚Üí Logs
```

**Sa√≠da esperada nos logs**:
```
Extraindo labels NER...
[EXTRA√çDO] cliente: 'jo√£o silva'
[EXTRA√çDO] produto: 'notebook'
[EXTRA√çDO] valor: '2500'
Registros v√°lidos: ~500/1000
Estat√≠sticas: invalid_id=~250, invalid_data=~250
Taxa de limpeza: ~50%
```

#### 3. Camada Gold (Aggregations)

**Onde verificar**:
- **MinIO Console**: http://localhost:9001
  - Bucket `gold`
  - Arquivo `gold_analytics_YYYYMMDD_HHMMSS.parquet`

**O que esperar**:
- KPIs agregados por cliente, produto, regi√£o
- Valores totais, m√©dias, contagens
- Dados prontos para dashboard

**Valida√ß√£o via script**:
```bash
python diagnose_data_flow.py
```

**Sa√≠da esperada**:
```
========================================
DIAGN√ìSTICO COMPLETO DO PIPELINE
========================================

CAMADA BRONZE:
  Arquivos: 1
  Registros: 1000

CAMADA SILVER:
  Arquivos: 1
  Registros: ~500
  Taxa de reten√ß√£o: ~50%

CAMADA GOLD:
  Arquivos: 1
  Registros: ~500
  KPIs calculados: 8
```

#### 4. Dashboard Streamlit

**Onde verificar**:
- **URL**: http://localhost:8501

**O que esperar**:
- **5 KPIs principais** no topo:
  - Receita Total
  - Total de Vendas
  - Clientes √önicos
  - Produtos √önicos
  - Ticket M√©dio

- **7 abas dispon√≠veis**:
  1. Vendas: Gr√°ficos de vendas por per√≠odo
  2. Clientes: Top clientes, distribui√ß√£o
  3. Produtos: Top produtos, categorias
  4. Geogr√°fico: Vendas por regi√£o/cidade
  5. Pagamento: M√©todos de pagamento
  6. Dados Brutos: Tabela completa export√°vel
  7. **Pipeline**: Monitoramento Bronze/Silver/Gold

**Aba Pipeline - O que validar**:
- Contagem de registros em cada camada
- Taxa de limpeza (% removidos Bronze ‚Üí Silver)
- Gr√°fico de funil mostrando fluxo de dados
- Timestamps de √∫ltima atualiza√ß√£o de cada camada
- Taxas de reten√ß√£o entre camadas

**Screenshot esperado**:
```
üìÇ Registros Bronze    ‚úÖ Registros Silver    ‚≠ê Registros Gold    üßπ Taxa de Limpeza
     1000                    ~500                   ~500               ~50.0%
```

### Logs e Monitoramento

#### Airflow Logs

**Acessar via UI**:
1. http://localhost:8080
2. Login: `airflow` / `airflow`
3. DAGs ‚Üí `00_event_driven_ingestion`
4. Graph View ‚Üí Clique em qualquer task ‚Üí Logs

**Acessar via Docker**:
```bash
# Logs do scheduler (onde DAGs executam)
docker-compose logs -f airflow-scheduler

# Logs de task espec√≠fica
docker-compose logs airflow-scheduler | grep "insert_bronze"
```

**O que procurar nos logs**:
- `[INFO]`: Execu√ß√µes bem-sucedidas
- `[ERROR]`: Falhas que precisam investiga√ß√£o
- `Task succeeded`: Task completada com sucesso
- `Poking for file`: Sensor aguardando arquivo

#### MinIO Logs

```bash
# Ver atividade de upload/download
docker-compose logs -f minio
```

#### Label Studio Logs

```bash
# Ver requisi√ß√µes API
docker-compose logs -f label-studio
```

## Reprodu√ß√£o do Cen√°rio Event-Driven

Este guia mostra como testar o fluxo completo event-driven do pipeline.

**V√≠deo demonstrativo**: https://drive.google.com/file/d/1MWBXpvQyZESNMVfSMm9WTBzPWJRU-WAS/view?usp=sharing

### Cen√°rio: Pipeline Detecta Novo Arquivo e Processa Automaticamente

#### Passo 1: Preparar Ambiente

```bash
# Garantir que containers est√£o rodando
docker-compose ps

# Todos devem estar "Up (healthy)"
# Se n√£o estiverem, execute:
docker-compose up -d

# Aguardar ~2min para inicializa√ß√£o completa
```

#### Passo 2: Limpar Buckets (Come√ßar do Zero)

```bash
# Ativar ambiente Python
conda activate dataops

# Limpar todos os buckets
python -m scripts_pipeline.clean_buckets
```

**Sa√≠da esperada**:
```
Limpando bucket bronze...
Limpando bucket silver...
Limpando bucket gold...
Limpeza conclu√≠da!
```

#### Passo 3: Verificar DAG no Airflow

1. Acesse http://localhost:8080
2. Login: `airflow` / `airflow`
3. Localize DAG: `00_event_driven_ingestion`
4. **Ative a DAG** (toggle no canto esquerdo deve ficar azul)

#### Passo 4: Verificar Sensor em Execu√ß√£o

Na interface do Airflow:
1. Clique na DAG `00_event_driven_ingestion`
2. Graph View
3. Voc√™ deve ver task `wait_for_bronze_file` em estado **running** (verde claro)
4. Clique na task ‚Üí Logs

**Logs esperados**:
```
[INFO] Poking for file s3://bronze/*.json
[INFO] Deferring task to triggerer
```

Isso significa que o sensor est√° **aguardando ativamente** por um arquivo JSON no bucket Bronze.

#### Passo 5: Triggerar o Evento (Upload para Bronze)

Agora vamos simular o evento: **upload de arquivo para Bronze**.

```bash
# Executar script de ingest√£o (simula aplica√ß√£o enviando dados)
python -m scripts_pipeline.insert_bronze
```

**Sa√≠da esperada**:
```
Conectando ao Label Studio...
Extraindo dados do projeto 3...
Enviando para bucket bronze...
Arquivo bronze_20250115_143022.json criado com sucesso!
1000 registros enviados para Bronze
```

#### Passo 6: Observar Pipeline Event-Driven em A√ß√£o

**O que acontece automaticamente**:

1. **Sensor detecta arquivo** (~30s ap√≥s upload):
   - Task `wait_for_bronze_file` muda para **SUCCESS** (verde)

2. **Pipeline executa sequencialmente**:
   - `transform_silver` inicia (amarelo ‚Üí verde)
   - `aggregate_gold` inicia ap√≥s Silver completar
   - `diagnose` executa verifica√ß√£o final

**Acompanhar em tempo real**:
1. Airflow UI ‚Üí Graph View (atualiza a cada 30s)
2. Ou clique em "Auto-refresh" para atualizar automaticamente

**Tempo total esperado**: 2-4 minutos do upload at√© conclus√£o

#### Passo 7: Validar Resultado Final

**Via Airflow**:
- Todas as tasks devem estar verdes (SUCCESS)
- Task `diagnose` mostra estat√≠sticas nos logs

**Via MinIO Console** (http://localhost:9001):
- Bronze: 1 arquivo JSON
- Silver: 1 arquivo Parquet
- Gold: 1 arquivo Parquet

**Via Dashboard** (http://localhost:8501):
- Aba "Pipeline" deve mostrar:
  - 1000 registros Bronze
  - ~500 registros Silver (metade removida por valida√ß√£o)
  - ~500 registros Gold
  - Taxa de limpeza ~50%

**Via CLI**:
```bash
python diagnose_data_flow.py
```

### Cen√°rio Alternativo: Trigger Manual

Se quiser executar o pipeline **sem esperar pelo sensor**:

```bash
# Op√ß√£o 1: Via Airflow UI
# Clique no bot√£o "Play" (‚ñ∂) na DAG

# Op√ß√£o 2: Via CLI
docker-compose exec airflow-webserver airflow dags trigger 00_event_driven_ingestion
```

### Valida√ß√£o do Event-Driven

**Como confirmar que √© event-driven de verdade?**

1. **Teste 1: Upload m√∫ltiplo**
   ```bash
   # Limpar buckets
   python -m scripts_pipeline.clean_buckets

   # Aguardar sensor detectar bucket vazio
   # Ent√£o fazer upload
   python -m scripts_pipeline.insert_bronze

   # Pipeline deve executar automaticamente
   ```

2. **Teste 2: Verificar modo deferrable**
   - Airflow logs do sensor devem mostrar: `Deferring task to triggerer`
   - Isso confirma que est√° usando deferrable mode (eficiente)

3. **Teste 3: M√∫ltiplas execu√ß√µes**
   - Ap√≥s primeira execu√ß√£o, limpe buckets novamente
   - Fa√ßa novo upload
   - Nova execu√ß√£o da DAG deve ser triggerada automaticamente

## Licen√ßa

Este projeto foi desenvolvido como trabalho de conclus√£o de disciplina.

## Contribuindo

Contribui√ß√µes s√£o bem-vindas! Por favor:
1. Fork o projeto
2. Crie uma branch para sua feature
3. Commit suas mudan√ßas
4. Push para a branch
5. Abra um Pull Request

## Suporte

- **Documenta√ß√£o**: Ver arquivos `.md` na raiz do projeto
- **Issues**: Abra uma issue no GitHub

---

**Desenvolvido usando Python, Airflow, Docker**
