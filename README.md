# üìä Pipeline de Monitoramento de C√¢mbio, Anota√ß√£o e BI Semanal
_Projeto DataOps com Airflow, Postgres, Datasets e Dynamic Task Mapping_

---

## ‚ú® Objetivo Geral

Este projeto implementa um pipeline **end-to-end** para monitorar o c√¢mbio **USD/BRL**, com foco em:

- Ingest√£o autom√°tica de dados via API p√∫blica  
- **Anota√ß√£o autom√°tica** dos dados (flag de alerta)  
- **Valida√ß√£o e Data Quality** com logging em tabela dedicada  
- **Gera√ß√£o de vis√£o de neg√≥cio semanal** em uma *view* SQL  
- **Relat√≥rios semanais enriquecidos** em tabela f√≠sica  
- **Monitoramento t√©cnico** do pipeline (status, erros, registros inseridos)  
- Uso de **Airflow Datasets (Data-Aware Scheduling)** para orquestra√ß√£o orientada a dados  
- Uso de **Dynamic Task Mapping** para gerar relat√≥rios semanais de forma escal√°vel

---

## üß© Vis√£o das DAGs

O pipeline √© composto por **duas DAGs**:

### 1. `monitoramento_cambio_anotacoes` (DAG principal)

Executa diariamente (`@daily`, com `catchup=True`) e √© respons√°vel por:

1. **Cria√ß√£o de tabelas** no Postgres (se n√£o existirem):
   - `fx_usdbrl_monitoramento` ‚Äì fato di√°ria do c√¢mbio  
   - `fx_dq_results` ‚Äì m√©tricas de Data Quality  
   - `bi_fx_monitoramento_pipeline` ‚Äì log t√©cnico das execu√ß√µes  
   - `fx_relatorios_semanais` ‚Äì tabela final de relat√≥rios semanais (consumida pela DAG 2)

2. **Ingest√£o de dados da API**  
   - Task: `get_exchange_rates`  
   - Fonte: `https://economia.awesomeapi.com.br/json/daily/USD-BRL/`  
   - Busca os dados do dia baseado em `logical_date` da execu√ß√£o da DAG.

3. **Anota√ß√£o autom√°tica**  
   - Task: `annotate_rates`  
   - Cria a flag:
     ```python
     alert_flag = 1 se bid > LIMITE_ALERTA
     ```
   - O limite √© configur√°vel:
     ```python
     LIMITE_ALERTA = 5.35
     ```

4. **Valida√ß√£o + UPSERT no Postgres**  
   - Task: `validate_and_save`  
   - Regras:
     - Se `bid` ou `ask` forem `None` ‚Üí registra linha ‚Äúnula‚Äù para o dia  
     - Se `bid <= 0` ou `ask <= 0` ‚Üí gera erro e loga falha  
     - Usa `ON CONFLICT (ref_date)` para garantir **idempot√™ncia** por dia:
       ```sql
       INSERT INTO fx_usdbrl_monitoramento (...)
       VALUES (...)
       ON CONFLICT (ref_date) DO UPDATE SET ...
       ```

   - **Emite o Dataset** `FX_MONITORAMENTO_DS`:
     ```python
     FX_MONITORAMENTO_DS = Dataset("postgres://fx_usdbrl_monitoramento")
     ```

5. **Data Quality**  
   - Task: `data_quality`  
   - L√™ `fx_usdbrl_monitoramento` e calcula:
     - `total_registros`
     - `erros_bid_nao_positivo`
     - `erros_ask_nao_positivo`
     - `nulos_bid`
     - `nulos_ask`
   - Insere tudo em `fx_dq_results`.

6. **BI ‚Äì vis√£o semanal**  
   - Task: `build_bi`  
   - Cria a *view* `bi_fx_cambio_negocio`, agregada por semana:
     ```sql
     CREATE OR REPLACE VIEW bi_fx_cambio_negocio AS
     SELECT
         date_trunc('week', ref_date)::date AS semana,
         AVG(bid) AS bid_medio_semana,
         AVG(ask) AS ask_medio_semana,
         SUM(CASE WHEN alert_flag = 1 THEN 1 ELSE 0 END) AS dias_alerta,
         COUNT(*) AS dias_com_dado,
         CASE
             WHEN COUNT(*) = 0 THEN 0
             ELSE SUM(CASE WHEN alert_flag = 1 THEN 1 ELSE 0 END)::float / COUNT(*)
         END AS pct_dias_alerta
     FROM fx_usdbrl_monitoramento
     GROUP BY date_trunc('week', ref_date)
     ORDER BY semana;
     ```

   - **Emite o Dataset** `FX_BI_SEMANAL_DS`:
     ```python
     FX_BI_SEMANAL_DS = Dataset("postgres://bi_fx_cambio_negocio")
     ```

7. **Monitoramento t√©cnico**  
   - Task: `monitor_pipeline`  
   - Registra em `bi_fx_monitoramento_pipeline`:
     - data_execucao  
     - task  
     - status  
     - mensagem_erro (se houver)  
     - `nova_linha_fato` (quantidade de registros inseridos/atualizados)

---

### 2. `fx_bi_relatorios_semanais` (DAG de relat√≥rios)

Essa DAG **n√£o tem schedule cron**.  
Ela √© disparada **automaticamente por Dataset**, quando a DAG 1 atualiza o BI semanal.

```python
with DAG(
    dag_id="fx_bi_relatorios_semanais",
    start_date=datetime(2025, 11, 1),
    schedule=[FX_BI_SEMANAL_DS],  # data-driven
    catchup=False,
    ...
)
```

Ela executa:

1. **Leitura das semanas dispon√≠veis**
   - Task: `get_semanas_unicas`
   - L√™ `bi_fx_cambio_negocio` e extrai:
     ```sql
     SELECT DISTINCT semana FROM bi_fx_cambio_negocio;
     ```
   - Retorna uma lista de semanas em formato string.

2. **Dynamic Task Mapping por semana**
   - Task: `gerar_relatorio_semana`
   - √â mapeada dinamicamente:
     ```python
     semanas = get_semanas_unicas()
     gerar_relatorio_semana.expand(semana=semanas)
     ```
   - Para cada semana:
     - L√™ a linha correspondente da *view* `bi_fx_cambio_negocio`
     - Calcula `risco_semana`:
       ```python
       risco_semana = 1 se pct_dias_alerta > 0.5, sen√£o 0
       ```
     - Insere/atualiza em `fx_relatorios_semanais`:
       ```sql
       INSERT INTO fx_relatorios_semanais (...)
       VALUES (...)
       ON CONFLICT (semana) DO UPDATE SET ...
       ```

---

## üóÑÔ∏è Estrutura das Tabelas Principais

### `fx_usdbrl_monitoramento`
- `id` (PK)  
- `ref_date` (DATE, UNIQUE)  
- `ref_timestamp` (TIMESTAMP)  
- `code`, `codein`, `name`  
- `bid`, `ask`, `pct_change`  
- `alert_flag` (INT)  
- `created_at`  

### `fx_dq_results`
- `data_execucao`  
- `total_registros`  
- `erros_bid_nao_positivo`  
- `erros_ask_nao_positivo`  
- `nulos_bid`  
- `nulos_ask`  

### `bi_fx_monitoramento_pipeline`
- `data_execucao`  
- `execution_date`  
- `task`  
- `status`  
- `mensagem_erro`  
- `nova_linha_fato`  

### `bi_fx_cambio_negocio` (VIEW)
- `semana`  
- `bid_medio_semana`  
- `ask_medio_semana`  
- `dias_alerta`  
- `dias_com_dado`  
- `pct_dias_alerta`  

### `fx_relatorios_semanais`
- `semana` (PK)  
- `bid_medio_semana`  
- `ask_medio_semana`  
- `dias_alerta`  
- `dias_com_dado`  
- `pct_dias_alerta`  
- `risco_semana` (INT: 1 se mais de 50% dos dias da semana foram alerta)  
- `created_at`  

---

## üîó Como Funcionam os Datasets e Eventos

Este projeto utiliza **Airflow Datasets** para orquestrar as DAGs de forma **data-driven**.

### Datasets definidos

```python
FX_MONITORAMENTO_DS = Dataset("postgres://fx_usdbrl_monitoramento")
FX_BI_SEMANAL_DS   = Dataset("postgres://bi_fx_cambio_negocio")
```

- `FX_MONITORAMENTO_DS` √© emitido pela task `validate_and_save`.  
- `FX_BI_SEMANAL_DS` √© emitido pela task `build_bi`.

### Emiss√£o de Dataset (outlets)

Exemplo em `validate_and_save`:

```python
@task(outlets=[FX_MONITORAMENTO_DS])
def validate_and_save(record: dict) -> int:
    ...
```

Exemplo em `build_bi`:

```python
@task(outlets=[FX_BI_SEMANAL_DS])
def build_bi():
    ...
```

### DAG data-driven (consumidora de Dataset)

A DAG `fx_bi_relatorios_semanais` √© configurada com:

```python
with DAG(
    dag_id="fx_bi_relatorios_semanais",
    schedule=[FX_BI_SEMANAL_DS],
    ...
)
```

Isso significa que:

- Ela **n√£o √© acionada por cron**  
- Ela **√© acionada automaticamente** sempre que a DAG 1 atualiza o Dataset `FX_BI_SEMANAL_DS` na execu√ß√£o de `build_bi`.

---

## ‚öôÔ∏è Como Acionar os Eventos/Datasets na Pr√°tica

### 1. Ativar as duas DAGs

No Airflow UI:

- Deixe **`monitoramento_cambio_anotacoes`** como `unpaused`  
- Deixe **`fx_bi_relatorios_semanais`** como `unpaused`  

> Se a DAG de relat√≥rios estiver `paused`, ela **n√£o** ser√° disparada pelo Dataset, mesmo que o Dataset seja atualizado.

---

### 2. Rodar a DAG principal e observar o disparo autom√°tico

1. No Airflow UI, clique em:
   - `monitoramento_cambio_anotacoes` ‚Üí *Run DAG*

2. A execu√ß√£o seguir√° a sequ√™ncia:
   - `create_tables`  
   - `get_exchange_rates`  
   - `annotate_rates`  
   - `validate_and_save` (emite `FX_MONITORAMENTO_DS`)  
   - `data_quality`  
   - `build_bi` (emite `FX_BI_SEMANAL_DS`)  
   - `monitor_pipeline`

3. Quando `build_bi` terminar com sucesso, o Airflow ir√°:
   - Marcar o Dataset `FX_BI_SEMANAL_DS` como atualizado
   - Disparar automaticamente a DAG `fx_bi_relatorios_semanais`

4. Na DAG 2, voc√™ ver√°:
   - `get_semanas_unicas`  
   - `gerar_relatorio_semana[semana_1]`  
   - `gerar_relatorio_semana[semana_2]`  
   - etc.

### 3. Teste manual de evento/dataset

Voc√™ tamb√©m pode testar apenas a parte de BI + Dataset:

- Execute manualmente **somente** a task `build_bi` da DAG 1:
  - Graph ‚Üí clique em `build_bi` ‚Üí *Run*

Isso:

- Recria a view `bi_fx_cambio_negocio`  
- Emite o Dataset `FX_BI_SEMANAL_DS`  
- Dispara automaticamente a DAG `fx_bi_relatorios_semanais`

---

## ‚úÖ Como Validar a Ingest√£o Autom√°tica Ponta a Ponta

Ap√≥s rodar o pipeline, voc√™ pode validar pelo banco (Metabase, Adminer, psql).

### 1. Validar ingest√£o e anota√ß√£o di√°ria

```sql
SELECT *
FROM fx_usdbrl_monitoramento
ORDER BY ref_date;
```

Verifique:

- Se h√° registros para os dias esperados  
- Se `bid`, `ask` e `alert_flag` est√£o preenchidos  
- Se o dia corrente est√° presente ap√≥s a execu√ß√£o da DAG

---

### 2. Validar Data Quality

```sql
SELECT *
FROM fx_dq_results
ORDER BY data_execucao DESC;
```

Verifique:

- `total_registros` crescendo ao longo do tempo  
- `erros_*` normalmente iguais a zero  
- `nulos_*` podem existir se API n√£o retornar dados em algum dia, principalmente dias n√£o-√∫teis

---

### 3. Validar a vis√£o de neg√≥cio semanal (view)

```sql
SELECT *
FROM bi_fx_cambio_negocio
ORDER BY semana;
```

Verifique:

- `bid_medio_semana` e `ask_medio_semana` fazendo sentido  
- `dias_alerta` coerente com a regra de `alert_flag`  
- `pct_dias_alerta` entre 0 e 1

---

### 4. Validar os relat√≥rios semanais enriquecidos (DAG 2)

```sql
SELECT *
FROM fx_relatorios_semanais
ORDER BY semana;
```

Verifique:

- As mesmas m√©tricas da view, agora **materializadas** em tabela  
- Campo `risco_semana`:
  - 1 se `pct_dias_alerta > 0.5` (mais da metade dos dias da semana em alerta)  
  - 0 caso contr√°rio  

Se essas linhas existem e foram preenchidas **sem voc√™ rodar manualmente a DAG 2**, ent√£o:

- O Dataset `FX_BI_SEMANAL_DS` foi emitido corretamente  
- A DAG `fx_bi_relatorios_semanais` foi disparada automaticamente  
- A ingest√£o est√° funcionando ponta a ponta, de forma **data-driven**

---

## üìä Sugest√µes de Dashboards (Metabase)

### Vis√£o de Neg√≥cio

Base: `bi_fx_cambio_negocio` ou `fx_relatorios_semanais`

- Linha: `bid_medio_semana` vs. `semana`  
- Linha: `ask_medio_semana` vs. `semana`  
- Barra: `dias_alerta` por semana  
- Indicador: `% semanas com risco_semana = 1`

### Monitoramento T√©cnico

Base: `bi_fx_monitoramento_pipeline` + `fx_dq_results`

- Cards:
  - Total execu√ß√µes  
  - M√©dia de linhas inseridas  
  - Execu√ß√µes com erro  
- S√©ries temporais:
  - `nova_linha_fato` ao longo do tempo  
  - contagem de erros de valida√ß√£o  
- Tabela:
  - √öltimas execu√ß√µes com `status`, `task`, `mensagem_erro`

---

## üèÅ Conclus√£o

Este projeto demonstra:

- Pr√°ticas de **DataOps** aplicadas na pr√°tica  
- Orquestra√ß√£o orientada a dados (**Airflow Datasets**)  
- Separa√ß√£o clara entre:
  - Pipeline de ingest√£o/transforma√ß√£o (DAG 1)  
  - Pipeline de BI/relat√≥rio (DAG 2)  
- Uso de **Dynamic Task Mapping** para escalar por semana  
- Observabilidade (tabelas de DQ e monitoramento)  
- Pipeline idempotente (UPSERT por dia)  
- BI atualizado de forma **autom√°tica**, sem interven√ß√£o manual