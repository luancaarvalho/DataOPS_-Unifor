<h1 align="center">DataOPS â€” Pipeline COVID-19 (Airflow + PostgreSQL + Grafana)</h1>

<p align="center">
  Pipeline <b>end-to-end</b> orquestrado em <b>Apache Airflow</b> para ingestÃ£o, transformaÃ§Ã£o, qualidade e visualizaÃ§Ã£o de dados de COVID-19
  (fonte: <code>brasil.io</code>), com armazenamento em <b>PostgreSQL</b> e dashboard pronto em <b>Grafana</b>.
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Airflow-2.10-blue" />
  <img src="https://img.shields.io/badge/PostgreSQL-13-336791" />
  <img src="https://img.shields.io/badge/Grafana-Latest-orange" />
  <img src="https://img.shields.io/badge/Docker-Compose-green" />
</p>

<hr/>

<h2>O que este projeto entrega</h2>
<ul>
  <li><b>IngestÃ£o</b> via API pÃºblica <code>https://brasil.io/covid19/cities/cases/</code>.</li>
  <li><b>TransformaÃ§Ã£o</b> com Pandas + cÃ¡lculo de mÃ©dia mÃ³vel de 7 dias.</li>
  <li><b>Qualidade</b> (checagem de nulos e valores negativos).</li>
  <li><b>Armazenamento</b> em PostgreSQL (tabela <code>covid_cases</code>).</li>
  <li><b>Dashboard Grafana</b> provisionado automaticamente (datasource + dashboard).</li>
  <li><b>OrquestraÃ§Ã£o</b> e rastreabilidade com Apache Airflow.</li>
</ul>

<hr/>

<h2>Arquitetura (visÃ£o geral)</h2>

<pre>
[brasil.io] --ingestÃ£o--> [CSV bruto]
                         \---> [TransformaÃ§Ã£o + Qualidade] --load--> [PostgreSQL] --(datasource)--> [Grafana]
                                                     ^
                                                     |
                                                  [Airflow DAG]
</pre>

<hr/>

<h2>Estrutura (trecho relevante)</h2>

<pre>
dataOps-Unifor/
â””â”€ airflow/
   â”œâ”€ dags/
   â”‚  â””â”€ covid_pipeline_dag.py
   â”œâ”€ include/
   â”‚  â”œâ”€ ingestion.py
   â”‚  â”œâ”€ transform.py
   â”‚  â””â”€ load_postgres.py
   â”œâ”€ grafana/
   â”‚  â”œâ”€ dashboards/
   â”‚  â”‚  â””â”€ covid_dashboard.json
   |  |  â””â”€ monitoramento.json
   â”‚  â””â”€ provisioning/
   â”‚     â”œâ”€ datasources.yml
   â”‚     â””â”€ dashboards/
   â”‚        â””â”€ covid-dashboard.yml
   â”œâ”€ data/
   â”‚  â”œâ”€ raw/               (csv bruto gerado pela ingestÃ£o)
   â”‚  â””â”€ processed/         (csv transformado)
   â”œâ”€ docker-compose.yaml
   â””â”€ Dockerfile
</pre>

<hr/>

<h2>PrÃ©-requisitos</h2>
<ul>
  <li>Docker Desktop (ou Docker Engine) + Docker Compose</li>
  <li>Portas livres: <code>8080</code> (Airflow), <code>3000</code> (Grafana)</li>
</ul>

<hr/>

<h2>Como subir o ambiente</h2>

<h3>1) Clone o repositÃ³rio</h3>

<pre><code>git clone https://github.com/dantedod/DataOPS_-Unifor/tree/trab-dante-rafael
cd DataOPS_-Unifor/trabalho-final-dataOps/airflow
</code></pre>

<h3>2) Suba os containers</h3>

<p>Com <b>Docker Compose</b>:</p>
<pre><code>docker compose up -d --build
</code></pre>

<p>Ou, se o repositÃ³rio tiver <code>Makefile</code> com alvo <code>up</code>:</p>
<pre><code>make up
</code></pre>

<h3>3) Acesse as UIs</h3>

<ul>
  <li><b>Airflow:</b> <code>http://localhost:8080</code> (usuÃ¡rio: <code>airflow</code>, senha: <code>airflow</code>)</li>
  <li><b>Grafana:</b> <code>http://localhost:3000</code> (usuÃ¡rio: <code>admin</code>, senha: <code>admin</code>)</li>
</ul>

<hr/>

<h2>Executando o pipeline</h2>

<ol>
  <li>Abra o Airflow e <b>ative</b> (unpause) a DAG <code>covid_pipeline_dag</code>.</li>
  <li><b>Trigger</b> (play) para executar imediatamente.</li>
  <li>O fluxo executa, na ordem:
    <ul>
      <li><code>wait_for_data</code> (sensor local) â€” aguarda o CSV bruto <code>/opt/airflow/data/raw/covid_data.csv</code>. No primeiro run, o arquivo serÃ¡ gerado pela ingestÃ£o e, nas prÃ³ximas execuÃ§Ãµes, o sensor passa direto se o arquivo jÃ¡ existir.</li>
      <li><code>ingest_data</code> â€” baixa dados da API e salva CSV bruto.</li>
      <li><code>validate_data</code> â€” checa nulos e valores negativos.</li>
      <li><code>transform_and_load</code> â€” transforma e grava na tabela <code>covid_cases</code> (PostgreSQL).</li>
      <li><code>notify_grafana</code> â€” registra que os dados jÃ¡ estÃ£o disponÃ­veis para o dashboard.</li>
    </ul>
  </li>
  <li>No Grafana, o dashboard <b>â€œMonitoramento COVID CearÃ¡â€</b> jÃ¡ estarÃ¡ provisionado:
    <ul>
      <li>Datasource: <b>covid_postgres</b> (conectado ao banco <code>airflow</code>, usuÃ¡rio <code>airflow</code>, senha <code>airflow</code>)</li>
      <li>Dashboard: em <i>Dashboards â†’ COVID Dashboards</i></li>
    </ul>
  </li>
</ol>

<hr/>

<h2>Tabela criada</h2>

<ul>
  <li><code>covid_cases</code> (no banco <code>airflow</code>)</li>
</ul>

<p>Colunas principais:</p>
<ul>
  <li><code>city</code>, <code>date</code>, <code>confirmed</code>, <code>deaths</code>, <code>rolling_avg</code></li>
</ul>

<p>Exemplo para inspecionar via psql:</p>

<pre><code>docker exec -it airflow-postgres-1 psql -U airflow -d airflow -c "SELECT city, date, confirmed, deaths FROM covid_cases LIMIT 10;"
</code></pre>

<hr/>

<h2>Dashboard (Grafana)</h2>

<ul> 
  <li>Datasource provisionado em <code>grafana/provisioning/datasources.yml</code> (uid: <code>covid_postgres</code>).</li> <li>Dashboards provisionados automaticamente ao iniciar o container Grafana:</li> <ul> <li><b>1ï¸âƒ£ Monitoramento COVID CearÃ¡</b> â€” dashboard de negÃ³cio, com a evoluÃ§Ã£o de casos confirmados por cidade, mÃ©dia mÃ³vel de 7 dias e dados histÃ³ricos carregados do PostgreSQL (<code>covid_cases</code>).</li> <li><b>2ï¸âƒ£ VisÃ£o TÃ©cnica: Monitoramento do Pipeline (DataOps)</b> â€” dashboard de observabilidade tÃ©cnica da DAG do Airflow (<code>covid_pipeline_dag</code>), contendo: <ul> <li>Status das DAG Runs (Ãºltimas 24h e todo o perÃ­odo);</li> <li>Taxa de sucesso/falhas das execuÃ§Ãµes;</li> <li>LatÃªncia mÃ©dia (tempo de execuÃ§Ã£o);</li> <li>Listagem de tarefas bem-sucedidas e falhadas.</li> </ul> </li> </ul> <li>Ambos os dashboards estÃ£o disponÃ­veis automaticamente em <b>Dashboards â†’ COVID Dashboards</b> no Grafana.</li> 
</ul>

<h3>(Opcional) Testar falha da DAG</h3>

<p>Se quiser simular um cenÃ¡rio de falha no pipeline (para validar o monitoramento tÃ©cnico no Grafana), basta editar o arquivo:</p>
airflow/include/ingestion.py

<p>E alterar a variÃ¡vel da URL da API para um endereÃ§o invÃ¡lido, por exemplo:</p>
url original (correta)
url = "https://brasil.io/covid19/cities/cases/"

url propositalmente incorreta (para gerar erro)
url = "https://brasil.io/api/inexistente/"

<p>Depois disso:</p> <ol> <li>Execute novamente a DAG <code>covid_pipeline_dag</code> no Airflow (Trigger DAG).</li> <li>A task <code>ingest_data</code> falharÃ¡, e o painel <b>â€œTarefas Falhadas (Ãšltimas 24h)â€</b> do Grafana refletirÃ¡ automaticamente o erro.</li> </ol> <p>Assim, o avaliador pode visualizar o comportamento real de observabilidade e falha do pipeline em tempo real.</p>

<hr/>

<h2>Como â€œforÃ§arâ€ um novo run</h2>

<ol>
  <li>No Airflow, clique em <b>Trigger DAG</b> na <code>covid_pipeline_dag</code>.</li>
  <li>Ou rode a ingestÃ£o manualmente dentro do container:
    <pre><code>docker exec -it airflow-airflow-scheduler-1 python /opt/airflow/include/ingestion.py</code></pre>
  </li>
</ol>

<hr/>

<h2>Parar e limpar</h2>

<p>Parar containers:</p>
<pre><code>docker compose down
</code></pre>

<p>Parar e <b>remover volumes</b> (inclui dados do Postgres e estado do Grafana â€“ cuidado):</p>
<pre><code>docker compose down -v
</code></pre>

<hr/>

<h2>Troubleshooting rÃ¡pido</h2>

<ul>
  <li><b>Sensor de arquivo falhando</b>: o projeto usa um sensor local customizado (nÃ£o depende do <code>fs_default</code>). Certifique-se de que o caminho no DAG Ã© <code>/opt/airflow/data/raw/covid_data.csv</code> e que o volume <code>../data:/opt/airflow/data</code> estÃ¡ montado no <code>docker-compose.yaml</code>.</li>
  <li><b>Falha ao conectar no Postgres</b>: garanta que o serviÃ§o <code>postgres</code> esteja <i>healthy</i> e que a string do <code>create_engine</code> (em <code>include/load_postgres.py</code>) Ã©
    <code>postgresql+psycopg2://airflow:airflow@postgres:5432/airflow</code>.</li>
  <li><b>Dashboard vazio</b>: verifique o <i>time range</i> do painel no Grafana e se a tabela <code>covid_cases</code> possui registros para o perÃ­odo selecionado.</li>
</ul>

<hr/>

<h2>Tecnologias</h2>

<table>
  <tr><td>OrquestraÃ§Ã£o</td><td>Apache Airflow 2.10</td></tr>
  <tr><td>IngestÃ£o</td><td>Requests</td></tr>
  <tr><td>TransformaÃ§Ã£o</td><td>Pandas</td></tr>
  <tr><td>Storage</td><td>PostgreSQL</td></tr>
  <tr><td>BI</td><td>Grafana</td></tr>
  <tr><td>Infra</td><td>Docker Compose</td></tr>
</table>

<hr/>

<h2>LicenÃ§a</h2>
<p>
  Projeto acadÃªmico â€” uso educacional.<br><br>
  Desenvolvido por:<br>
  ğŸ‘‰ <a href="https://www.linkedin.com/in/dantedod/" target="_blank">Dante Dantas (LinkedIn)</a><br>
  ğŸ‘‰ <a href="https://www.linkedin.com/in/rafaeld3v/" target="_blank">Rafael Tavares (LinkedIn)</a>
</p> 