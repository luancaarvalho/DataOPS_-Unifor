"""
PyAirbyte Pipeline with Data Quality: GitHub ‚Üí Bronze ‚Üí Silver
================================================================
Este pipeline extrai dados do GitHub, valida qualidade e promove para Silver

Fluxo:
1. GitHub API ‚Üí PyAirbyte extraction
2. Salvar em Bronze (raw data)
3. Executar Data Quality checks
4. Se DQ passar: Promover para Silver (validated data)
"""

import airbyte as ab
import os
import boto3
import gzip
import json
import shutil
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any
from collections import Counter


class GitHubToMinIOPipeline:
    """Pipeline completo com Data Quality"""
    
    def __init__(self, github_token: str, repository: str):
        self.github_token = github_token
        self.repository = repository
        self.s3_client = self._init_s3_client()
        self.temp_dir = Path("/tmp/pyairbyte_export")
        self.temp_dir.mkdir(exist_ok=True)
        
        # M√©tricas do pipeline
        self.metrics = {
            'extraction': {},
            'data_quality': {},
            'promotion': {}
        }
    
    def _init_s3_client(self):
        """Inicializa cliente S3 para MinIO"""
        return boto3.client(
            's3',
            endpoint_url='http://localhost:9010',
            aws_access_key_id='minioadmin',
            aws_secret_access_key='minioadmin',
            region_name='us-east-1'
        )
    
    def extract_from_github(self, streams: List[str]) -> ab.ReadResult:
        """Extrai dados do GitHub usando PyAirbyte"""
        print("="*60)
        print("üì• FASE 1: EXTRA√á√ÉO DO GITHUB")
        print("="*60)
        
        # Configurar source
        print(f"\nüì¶ Reposit√≥rio: {self.repository}")
        print(f"üìã Streams: {', '.join(streams)}")
        
        source = ab.get_source(
            "source-github",
            config={
                "credentials": {"personal_access_token": self.github_token},
                "repositories": [self.repository],
                "start_date": "2024-01-01T00:00:00Z",
            },
            install_if_missing=True,
        )
        
        # Verificar conex√£o
        source.check()
        print("‚úÖ GitHub conectado!")
        
        # Selecionar streams
        source.select_streams(streams)
        
        # Extrair dados
        print("\n‚è≥ Extraindo dados...")
        result = source.read()
        
        # Coletar m√©tricas
        for stream_name, records in result.streams.items():
            count = len(list(records))
            self.metrics['extraction'][stream_name] = count
            print(f"  ‚Ä¢ {stream_name}: {count} registros")
         
        return result
    
    def save_to_bronze(self, result: ab.ReadResult) -> Dict[str, str]:
        """Salva dados extra√≠dos no bucket Bronze do MinIO"""
        print("\n" + "="*60)
        print("üíæ FASE 2: SALVANDO EM BRONZE (RAW DATA)")
        print("="*60)
        
        # Verificar conex√£o
        try:
            self.s3_client.head_bucket(Bucket='bronze')
            print("‚úÖ MinIO Bronze bucket acess√≠vel!")
        except Exception as e:
            print(f"‚ùå Erro ao acessar Bronze: {e}")
            raise
        
        timestamp = datetime.now().strftime("%Y_%m_%d_%H%M%S")
        uploaded_files = {}
        
        for stream_name, records in result.streams.items():
            record_list = list(records)
            
            if len(record_list) == 0:
                print(f"  ‚è≠Ô∏è  {stream_name}: vazio, pulando")
                continue
            
            # Preparar arquivo
            filename = f"{stream_name}_{timestamp}.jsonl.gz"
            local_path = self.temp_dir / filename
            s3_key = f"github_data/{stream_name}/{filename}"
            
            # Escrever JSONL comprimido
            with gzip.open(local_path, 'wt', encoding='utf-8') as f:
                for record in record_list:
                    # Converter para dict
                    if hasattr(record, '__dict__'):
                        record_dict = record.__dict__
                    elif hasattr(record, 'to_dict'):
                        record_dict = record.to_dict()
                    else:
                        record_dict = dict(record) if not isinstance(record, dict) else record
                    
                    # Estrutura Airbyte
                    airbyte_record = {
                        "_airbyte_raw_id": str(record_dict.get("_airbyte_raw_id", "")),
                        "_airbyte_extracted_at": str(record_dict.get("_airbyte_extracted_at", "")),
                        "_airbyte_data": record_dict
                    }
                    f.write(json.dumps(airbyte_record, default=str) + '\n')
            
            # Upload para Bronze
            try:
                self.s3_client.upload_file(str(local_path), 'bronze', s3_key)
                uploaded_files[stream_name] = s3_key
                print(f"  ‚úÖ {stream_name}: {len(record_list)} registros ‚Üí bronze/{s3_key}")
            except Exception as e:
                print(f"  ‚ùå Erro no upload de {stream_name}: {e}")
                raise
            
            # Manter arquivo local para DQ checks
        
        return uploaded_files
    
    def run_data_quality_checks(self, bronze_files: Dict[str, str]) -> Dict[str, Dict]:
        """Executa verifica√ß√µes de qualidade nos dados Bronze"""
        print("\n" + "="*60)
        print("üîç FASE 3: DATA QUALITY CHECKS")
        print("="*60)
        
        dq_results = {}
        
        for stream_name, s3_key in bronze_files.items():
            print(f"\nüìä Verificando qualidade: {stream_name}")
            print("-" * 40)
            
            # Baixar arquivo do Bronze
            local_file = self.temp_dir / Path(s3_key).name
            
            # Carregar dados
            records = []
            with gzip.open(local_file, 'rt', encoding='utf-8') as f:
                for line in f:
                    records.append(json.loads(line.strip()))
            
            # Executar checks
            errors = []
            warnings = []
            
            # 1. Schema compliance
            required_fields = ['_airbyte_raw_id', '_airbyte_extracted_at', '_airbyte_data']
            schema_issues = 0
            for idx, record in enumerate(records):
                missing = [f for f in required_fields if f not in record]
                if missing:
                    schema_issues += 1
                    errors.append({
                        'type': 'SCHEMA_VIOLATION',
                        'record_index': idx,
                        'missing_fields': missing
                    })
            
            if schema_issues == 0:
                print(f"  ‚úÖ Schema: todos os {len(records)} registros v√°lidos")
            else:
                print(f"  ‚ùå Schema: {schema_issues} viola√ß√µes")
            
            # 2. Duplicates
            ids = [r.get('_airbyte_raw_id') for r in records]
            id_counts = Counter(ids)
            duplicates = {rid: cnt for rid, cnt in id_counts.items() if cnt > 1}
            
            if not duplicates:
                print(f"  ‚úÖ Duplicatas: nenhuma encontrada")
            else:
                print(f"  ‚ùå Duplicatas: {len(duplicates)} IDs duplicados")
                for rid, cnt in duplicates.items():
                    errors.append({
                        'type': 'DUPLICATE_ID',
                        'raw_id': rid,
                        'count': cnt
                    })
            
            # 3. Completeness
            total = len(records)
            null_counts = {}
            for record in records:
                data = record.get('_airbyte_data', {})
                for key, value in data.items():
                    if value is None or value == "":
                        null_counts[key] = null_counts.get(key, 0) + 1
            
            completeness = {}
            for field, null_count in null_counts.items():
                pct = ((total - null_count) / total) * 100 if total > 0 else 0
                completeness[field] = round(pct, 2)
                if pct < 80:
                    warnings.append({
                        'type': 'LOW_COMPLETENESS',
                        'field': field,
                        'completeness_pct': pct
                    })
            
            if null_counts:
                print(f"  üìä Completude: {len(null_counts)} campos com valores nulos")
            else:
                print(f"  ‚úÖ Completude: 100% em todos os campos")
            
            # 4. Freshness
            now = datetime.now()
            ages = []
            for record in records:
                extracted_at_str = record.get('_airbyte_extracted_at', '')
                try:
                    extracted_at = datetime.fromisoformat(extracted_at_str.replace('Z', '+00:00'))
                    if extracted_at.tzinfo:
                        extracted_at = extracted_at.replace(tzinfo=None)
                    age_hours = (now - extracted_at).total_seconds() / 3600
                    ages.append(age_hours)
                except:
                    pass
            
            if ages:
                avg_age = sum(ages) / len(ages)
                print(f"  ‚úÖ Freshness: idade m√©dia {round(avg_age, 2)}h")
            
            # Calcular quality score
            total_checks = len(records)
            total_errors = len(errors)
            quality_score = ((total_checks - total_errors) / total_checks * 100) if total_checks > 0 else 0
            
            dq_results[stream_name] = {
                'total_records': len(records),
                'errors': errors,
                'warnings': warnings,
                'quality_score': round(quality_score, 2),
                'completeness': completeness,
                'passed': len(errors) == 0
            }
            
            # Salvar relat√≥rio DQ
            report_key = f"dq_reports/{stream_name}/{Path(s3_key).stem}_report.json"
            report_data = {
                'timestamp': datetime.now().isoformat(),
                'stream': stream_name,
                'bronze_file': s3_key,
                'results': dq_results[stream_name]
            }
            
            report_local = self.temp_dir / f"{stream_name}_dq_report.json"
            with open(report_local, 'w') as f:
                json.dump(report_data, f, indent=2)
            
            self.s3_client.upload_file(str(report_local), 'logs', report_key)
            report_local.unlink()
            
            print(f"  üìÑ Relat√≥rio DQ ‚Üí logs/{report_key}")
            
            if dq_results[stream_name]['passed']:
                print(f"  ‚úÖ Quality Score: {quality_score:.2f}/100 - APROVADO")
            else:
                print(f"  ‚ùå Quality Score: {quality_score:.2f}/100 - REPROVADO")
        
        self.metrics['data_quality'] = dq_results
        return dq_results
    
    def promote_to_silver(self, bronze_files: Dict[str, str], dq_results: Dict[str, Dict]) -> Dict[str, str]:
        """Promove dados validados de Bronze para Silver"""
        print("\n" + "="*60)
        print("ü•à FASE 4: PROMO√á√ÉO PARA SILVER (VALIDATED DATA)")
        print("="*60)
        
        silver_files = {}
        
        for stream_name, bronze_key in bronze_files.items():
            dq_result = dq_results.get(stream_name, {})
            
            if not dq_result.get('passed', False):
                print(f"  ‚è≠Ô∏è  {stream_name}: DQ falhou, n√£o promovendo para Silver")
                continue
            
            # Copiar de Bronze para Silver
            bronze_filename = Path(bronze_key).name
            silver_key = f"github_data/{stream_name}/{bronze_filename}"
            
            try:
                # Copiar objeto
                copy_source = {'Bucket': 'bronze', 'Key': bronze_key}
                self.s3_client.copy_object(
                    CopySource=copy_source,
                    Bucket='silver',
                    Key=silver_key
                )
                
                silver_files[stream_name] = silver_key
                
                # Adicionar metadados de promo√ß√£o
                metadata_key = f"github_data/{stream_name}/_metadata/{Path(bronze_filename).stem}.json"
                metadata = {
                    'promoted_at': datetime.now().isoformat(),
                    'source_bucket': 'bronze',
                    'source_key': bronze_key,
                    'quality_score': dq_result.get('quality_score'),
                    'record_count': dq_result.get('total_records'),
                    'promoted_by': 'pyairbyte_pipeline'
                }
                
                self.s3_client.put_object(
                    Bucket='silver',
                    Key=metadata_key,
                    Body=json.dumps(metadata, indent=2),
                    ContentType='application/json'
                )
                
                print(f"  ‚úÖ {stream_name}: {dq_result['total_records']} registros ‚Üí silver/{silver_key}")
                print(f"     Quality Score: {dq_result['quality_score']:.2f}/100")
                
            except Exception as e:
                print(f"  ‚ùå Erro ao promover {stream_name}: {e}")
        
        self.metrics['promotion'] = {
            'total_streams': len(bronze_files),
            'promoted_streams': len(silver_files),
            'promotion_rate': round(len(silver_files) / len(bronze_files) * 100, 2) if bronze_files else 0
        }
        
        return silver_files
    
    def cleanup(self):
        """Limpa arquivos tempor√°rios"""
        shutil.rmtree(self.temp_dir, ignore_errors=True)
    
    def print_summary(self):
        """Imprime resumo do pipeline"""
        print("\n" + "="*60)
        print("üìä RESUMO DO PIPELINE")
        print("="*60)
        
        # Extra√ß√£o
        print("\nüì• Extra√ß√£o:")
        total_extracted = sum(self.metrics['extraction'].values())
        print(f"  ‚Ä¢ Total de registros: {total_extracted}")
        for stream, count in self.metrics['extraction'].items():
            print(f"    - {stream}: {count}")
        
        # Data Quality
        print("\nüîç Data Quality:")
        dq = self.metrics['data_quality']
        passed = sum(1 for r in dq.values() if r.get('passed', False))
        total = len(dq)
        print(f"  ‚Ä¢ Streams aprovados: {passed}/{total}")
        for stream, result in dq.items():
            status = "‚úÖ PASS" if result.get('passed') else "‚ùå FAIL"
            score = result.get('quality_score', 0)
            print(f"    - {stream}: {status} (Score: {score:.2f}/100)")
        
        # Promo√ß√£o
        print("\nü•à Promo√ß√£o para Silver:")
        promo = self.metrics['promotion']
        print(f"  ‚Ä¢ Taxa de promo√ß√£o: {promo.get('promotion_rate', 0):.2f}%")
        print(f"  ‚Ä¢ Streams promovidos: {promo.get('promoted_streams', 0)}/{promo.get('total_streams', 0)}")
        
        print("\n" + "="*60)
        print("‚úÖ Pipeline conclu√≠do!")
        print("="*60)
        print(f"\nüåê MinIO Console: http://localhost:9011")
        print(f"üìÇ Bronze (raw): bronze/github_data/")
        print(f"üìÇ Silver (validated): silver/github_data/")
        print(f"üìÇ DQ Reports: logs/dq_reports/")
    
    def run(self, streams: List[str]):
        """Executa pipeline completo"""
        try:
            # 1. Extrair do GitHub
            result = self.extract_from_github(streams)
            
            # 2. Salvar em Bronze
            bronze_files = self.save_to_bronze(result)
            
            # 3. Data Quality Checks
            dq_results = self.run_data_quality_checks(bronze_files)
            
            # 4. Promover para Silver
            silver_files = self.promote_to_silver(bronze_files, dq_results)
            
            # 5. Resumo
            self.print_summary()
            
            return True
            
        except Exception as e:
            print(f"\n‚ùå Erro no pipeline: {e}")
            import traceback
            traceback.print_exc()
            return False
        
        finally:
            self.cleanup()


# Execu√ß√£o
if __name__ == "__main__":
    # Configura√ß√µes
    GITHUB_TOKEN = os.getenv("GITHUB_TOKEN", "your_token_here")
    REPO = "luancaarvalho/DataOPS_-Unifor"
    STREAMS = ["issues", "pull_requests", "stargazers", "commits"]
    
    print("üöÄ PyAirbyte Pipeline com Data Quality")
    print(f"üì¶ Reposit√≥rio: {REPO}")
    print("="*60)
    
    # Criar e executar pipeline
    pipeline = GitHubToMinIOPipeline(
        github_token=GITHUB_TOKEN,
        repository=REPO
    )
    
    success = pipeline.run(STREAMS)
    
    if success:
        print("\n‚ú® Pipeline executado com sucesso!")
    else:
        print("\n‚ùå Pipeline falhou. Verifique os logs acima.")
        exit(1)
