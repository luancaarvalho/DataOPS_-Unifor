# Instala√ß√£o Completa do Ambiente - Do Zero ao Pipeline Funcionando

Este guia cobre desde a instala√ß√£o do Conda at√© a execu√ß√£o completa do pipeline.

## üöÄ Instala√ß√£o R√°pida com Makefile (Recomendado)

Se voc√™ j√° tem **make**, **conda** e **docker** instalados, pode automatizar todo o setup:

```bash
# Setup completo automatizado (cria ambiente + sobe containers)
make first-run

# Ativar ambiente
conda activate dataops

# Executar pipeline
make pipeline

# Ver dashboard
make dashboard
```

**Ver todos os comandos dispon√≠veis**: `make help`

---

## üìñ Instala√ß√£o Manual Passo a Passo

Se voc√™ prefere entender cada etapa ou n√£o tem make instalado, siga o guia detalhado abaixo.

## Pr√©-requisitos

- **Sistema Operacional**: Windows 10/11, Linux ou macOS
- **RAM**: M√≠nimo 8GB (recomendado 16GB)
- **Espa√ßo em Disco**: M√≠nimo 10GB livres
- **Conex√£o com Internet**: Para download de depend√™ncias

---

## Parte 1: Instalar Conda (se n√£o tiver)

### Op√ß√£o A: Instalar Miniconda (Recomendado - Mais leve)

#### Windows:
1. **Baixe o instalador**:
   - Acesse: https://docs.conda.io/en/latest/miniconda.html
   - Baixe: `Miniconda3 Windows 64-bit`

2. **Execute o instalador**:
   - Duplo clique no arquivo `.exe` baixado
   - Aceite os termos
   - Escolha "Just Me" (recomendado)
   - Deixe o caminho padr√£o: `C:\Users\<seu_usuario>\miniconda3`
   - **IMPORTANTE**: Marque "Add Miniconda3 to my PATH environment variable"
   - Clique em "Install"

3. **Verifique a instala√ß√£o**:
   ```bash
   # Abra um novo terminal (PowerShell ou CMD)
   conda --version
   # Esperado: conda 23.x.x ou superior
   ```

#### Linux/Mac:
```bash
# Baixar instalador
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh

# Executar instalador
bash Miniconda3-latest-Linux-x86_64.sh

# Seguir instru√ß√µes no terminal
# Aceitar licen√ßa: yes
# Confirmar localiza√ß√£o: Enter
# Inicializar conda: yes

# Recarregar shell
source ~/.bashrc

# Verificar
conda --version
```

### Op√ß√£o B: Instalar Anaconda (Completo - Mais pesado)

1. Acesse: https://www.anaconda.com/products/distribution
2. Baixe o instalador para seu sistema operacional
3. Execute e siga as instru√ß√µes
4. Verifique: `conda --version`

---

## Parte 2: Criar Ambiente Conda

### 2.1 Criar ambiente com Python 3.10

```bash
# Criar ambiente conda chamado "dataops" com Python 3.10
conda create -n dataops python=3.10 -y

# Aguarde o download e instala√ß√£o (pode demorar 2-5 minutos)
```

### 2.2 Ativar o ambiente

```bash
# Windows (CMD ou PowerShell)
conda activate dataops

# Linux/Mac
conda activate dataops
```

**Voc√™ saber√° que funcionou quando ver**:
```
(dataops) C:\Users\seu_usuario>
          ‚Üë
    Ambiente ativado!
```

### 2.3 Verificar Python instalado

```bash
python --version
# Esperado: Python 3.10.x
```

---

## Parte 3: Instalar UV (Gerenciador de Depend√™ncias)

UV √© um gerenciador de pacotes Python extremamente r√°pido, escrito em Rust.

### 3.1 Instalar UV

#### Windows (PowerShell):
```bash
# M√©todo 1: Via pip
pip install uv

# OU M√©todo 2: Via instalador oficial
powershell -c "irm https://astral.sh/uv/install.ps1 | iex"
```

#### Linux/Mac:
```bash
# M√©todo 1: Via pip
pip install uv

# OU M√©todo 2: Via curl
curl -LsSf https://astral.sh/uv/install.sh | sh
```

### 3.2 Verificar instala√ß√£o

```bash
uv --version
# Esperado: uv 0.x.x ou superior
```

---

## Parte 4: Instalar Depend√™ncias do Projeto

### 4.1 Navegar at√© o diret√≥rio do projeto

```bash
cd d:\Projetos\Dataops
```

### 4.2 Instalar depend√™ncias com UV

```bash
# Sincronizar ambiente com as depend√™ncias do pyproject.toml
uv sync --directory environments

# OU se preferir usar pip tradicional (mais lento)
cd environments
pip install -e .
cd ..
```

**O que acontece**:
- UV l√™ `environments/pyproject.toml`
- Baixa e instala todas as depend√™ncias listadas:
  - pandas, numpy, sqlalchemy
  - Apache Airflow + providers
  - MinIO, boto3
  - Streamlit, Plotly
  - Label Studio SDK
  - E muitas outras...

**Tempo estimado**: 3-10 minutos (dependendo da conex√£o)

### 4.3 Verificar instala√ß√£o das depend√™ncias principais

```bash
# Verificar Airflow
python -c "import airflow; print(f'Airflow {airflow.__version__}')"

# Verificar Pandas
python -c "import pandas; print(f'Pandas {pandas.__version__}')"

# Verificar MinIO
python -c "import minio; print('MinIO SDK instalado!')"

# Verificar Streamlit
streamlit --version
```

---

## Parte 5: Instalar Docker e Docker Compose

### 5.1 Instalar Docker Desktop

#### Windows/Mac:
1. **Baixe Docker Desktop**:
   - Windows: https://www.docker.com/products/docker-desktop
   - Mac: https://www.docker.com/products/docker-desktop

2. **Execute o instalador**:
   - Duplo clique no arquivo baixado
   - Siga as instru√ß√µes padr√£o
   - **Windows**: Certifique-se de habilitar WSL 2 se solicitado

3. **Inicie o Docker Desktop**:
   - Abra o aplicativo Docker Desktop
   - Aguarde aparecer "Docker is running"

#### Linux:
```bash
# Ubuntu/Debian
curl -fsSL https://get.docker.com -o get-docker.sh
sudo sh get-docker.sh

# Adicionar seu usu√°rio ao grupo docker
sudo usermod -aG docker $USER

# Reiniciar sess√£o ou executar
newgrp docker

# Instalar Docker Compose
sudo apt-get install docker-compose-plugin
```

### 5.2 Verificar instala√ß√£o

```bash
# Verificar Docker
docker --version
# Esperado: Docker version 20.10.x ou superior

# Verificar Docker Compose
docker-compose --version
# Esperado: Docker Compose version v2.x.x ou superior

# Teste b√°sico
docker run hello-world
# Esperado: mensagem "Hello from Docker!"
```

---

## Parte 6: Configurar Vari√°veis de Ambiente

### 6.1 Criar arquivo .env

```bash
# Copiar template
cp .env.example .env
```

### 6.2 Editar .env

Abra o arquivo `.env` e preencha com suas credenciais:

```bash
# Windows
notepad .env

# Linux/Mac
nano .env
```

**Conte√∫do m√≠nimo do .env**:
```env
# ========================================
# MinIO Configuration
# ========================================
MINIO_ACCESS_KEY=admin_dataops
MINIO_SECRET_KEY=SenhaSegura123!MinIO

# ========================================
# Label Studio Configuration
# ========================================
LABELSTUDIO_TOKEN=  # Voc√™ vai preencher isso no Passo 8
LABELSTUDIO_PROJECT=4

# ========================================
# Airflow Configuration
# ========================================
AIRFLOW_UID=50000
```

**Salve o arquivo** (Ctrl+S no Notepad, Ctrl+X no nano)

---

## Parte 7: Iniciar Containers Docker

### 7.1 Build das imagens

```bash
# Certifique-se de estar no diret√≥rio do projeto
cd d:\Projetos\Dataops

# Build de todas as imagens
docker-compose build

# Tempo estimado: 5-15 minutos na primeira vez
```

### 7.2 Iniciar todos os servi√ßos

```bash
# Iniciar em background
docker-compose up -d

# Acompanhar logs (opcional)
docker-compose logs -f
```

### 7.3 Verificar status

```bash
# Verificar containers
docker-compose ps
```

**Esperado**:
```
NAME                    STATUS
airflow-webserver       Up (healthy)
airflow-scheduler       Up (healthy)
airflow-triggerer       Up (healthy)
minio                   Up (healthy)
label-studio            Up (healthy)
streamlit-dashboard     Up (healthy)
```

> **Aguarde**: Pode demorar 2-3 minutos at√© todos ficarem "healthy"

---

## Parte 8: Configurar Label Studio e Obter Token

### 8.1 Acessar Label Studio

```
http://localhost:8001
```

### 8.2 Criar conta (primeiro acesso)

1. **Clique em "Sign Up"**
2. **Preencha**:
   - Email: `label_ops@gmail.com`
   - Password: `dataops@123`
   - Confirm Password: `dataops@123`
3. **Clique em "Create Account"**

### 8.3 Configurar e Obter Legacy Token

**ATEN√á√ÉO**: O pipeline precisa do **LEGACY TOKEN**, n√£o do Access Token normal!

**Tutorial em v√≠deo**: https://drive.google.com/file/d/11teN7OjPgbhWD17H0z4XPJ5pYhE3D4_j/view?usp=sharing

**Passo A: Habilitar Legacy Tokens (evitar expira√ß√£o)**

1. **No Label Studio, clique em "Organization"** (menu lateral esquerdo)
2. **Clique em "API Tokens Settings"**
3. **Desmarque todas as flags EXCETO "Legacy tokens"**
   - Deixe APENAS "Legacy tokens" marcado
   - Desmarque as outras op√ß√µes (isso evita que o token expire)
4. **Clique em "Save"**

**Passo B: Copiar o Legacy Token**

1. **Clique no √≠cone do usu√°rio** (canto superior direito)
2. **Clique em "Account & Settings"**
3. **Procure "Legacy API Token"**
4. **Copie o token** (40 caracteres hexadecimais)

**Passo C: Obter ID do Projeto**

1. **Acesse o projeto no Label Studio**
2. **Veja a URL no navegador**: `http://localhost:8001/projects/3/data?tab=3`
3. **O n√∫mero ap√≥s `/projects/` √© o ID do projeto** (neste exemplo: `3`)

> **IMPORTANTE**:
> - A configura√ß√£o em "Organization > API Tokens Settings" garante que o token n√£o expire automaticamente
> - **Voc√™ DEVE inserir tanto o token quanto o ID do projeto no arquivo .env**

### 8.4 Atualizar .env com token e project ID

```bash
# Editar .env novamente
notepad .env  # Windows
nano .env     # Linux/Mac
```

**IMPORTANTE: Atualize AMBOS os valores**:
```env
LABELSTUDIO_TOKEN=a1b2c3d4e5f6g7h8i9j0k1l2m3n4o5p6q7r8s9t0  # Cole seu token aqui
LABELSTUDIO_PROJECT=3  # Cole o ID do projeto que aparece na URL
```

### 8.5 Reiniciar containers

```bash
docker-compose restart airflow-scheduler
docker-compose restart airflow-webserver
docker-compose restart streamlit
```

### 8.6 Importar Dataset do Projeto

O projeto utiliza um dataset de transa√ß√µes comerciais j√° anotado com entidades NER.

**Dataset dispon√≠vel em**:
```
https://drive.google.com/drive/folders/1WFkw54HojR1y_Io26_cNV5ni3888I2FZ?usp=sharing
```

**Como importar**:

1. **Baixe o dataset** do Google Drive
2. **Acesse Label Studio**: http://localhost:8001
3. **Abra o projeto** (ID 4)
4. **Clique em "Import"**
5. **Selecione os arquivos JSON** baixados
6. **Clique em "Import"**

**O que cont√©m o dataset**:
- Transa√ß√µes comerciais com dados de clientes, produtos e valores
- Anota√ß√µes NER j√° realizadas (cliente, produto, valor, etc.)
- Dados prontos para processamento pelo pipeline

> **IMPORTANTE**: Ap√≥s importar o dataset, voc√™ pode executar o pipeline para processar esses dados atrav√©s das camadas Bronze ‚Üí Silver ‚Üí Gold.

---

## Parte 9: Verificar Instala√ß√£o Completa

### 9.1 Verificar ambiente Python

```bash
# Ambiente ativado?
conda env list
# Deve mostrar * ao lado de "dataops"

# Pacotes instalados?
conda list | grep airflow
conda list | grep pandas
```

### 9.2 Verificar Docker

```bash
# Todos os containers rodando?
docker-compose ps

# Ver logs se algum estiver com problema
docker-compose logs <nome_do_container>
```

### 9.3 Acessar cada servi√ßo

- **Airflow**: http://localhost:8080 (airflow/airflow)
- **Label Studio**: http://localhost:8001 (label_ops@gmail.com/dataops@123)
- **MinIO**: http://localhost:9001 (seu MINIO_ACCESS_KEY / MINIO_SECRET_KEY)
- **Streamlit**: http://localhost:8501

---

## Parte 10: Executar Pipeline Pela Primeira Vez

### Op√ß√£o A: Via Airflow (Recomendado)

1. **Acesse Airflow**: http://localhost:8080
2. **Login**: airflow / airflow
3. **Ative a DAG**: `00_event_driven_ingestion`
4. **Trigger manualmente**: Clique no √≠cone de "play"
5. **Acompanhe execu√ß√£o**: Veja tasks sendo executadas

### Op√ß√£o B: Executar Scripts Localmente

```bash
# Certifique-se de que o ambiente conda est√° ativado
conda activate dataops

# Executar pipeline completo
python -m scripts_pipeline.clean_buckets
python -m scripts_pipeline.insert_bronze
python -m scripts_pipeline.transform_silver
python -m scripts_pipeline.aggregate_gold

# Verificar diagn√≥stico
python diagnose_data_flow.py

# Executar dashboard para visualizar os dados
streamlit run streamlit\dashboard.py
```

**Sa√≠da esperada do pipeline**:
```
  Detectado execu√ß√£o local. Usando endpoint: localhost:9000

  Extraindo labels NER...

   [EXTRA√çDO] cliente: 'jo√£o silva'
   [EXTRA√çDO] valor: '150.50'

Pipeline executado com sucesso!


**Sa√≠da esperada do Streamlit**:
```
You can now view your Streamlit app in your browser.

  Local URL: http://localhost:8501
```

> **IMPORTANTE**: O comando `streamlit run` deve ser executado AP√ìS os dados subirem para a camada Gold. O dashboard ficar√° rodando continuamente - pressione Ctrl+C para parar.

---

## Parte 11: Visualizar no Dashboard

### Op√ß√£o A: Dashboard no Docker (Autom√°tico)

Se voc√™ est√° usando Docker, o dashboard j√° est√° rodando automaticamente:
```
http://localhost:8501
```

### Op√ß√£o B: Dashboard Local (Execu√ß√£o Manual)

Se voc√™ executou o pipeline localmente (fora do Docker), ap√≥s os dados subirem para a camada Gold, execute:

```bash
# Certifique-se de que o ambiente conda est√° ativado
conda activate dataops

# Execute o dashboard Streamlit
streamlit run streamlit\dashboard.py
```

**Sa√≠da esperada**:
```
You can now view your Streamlit app in your browser.

  Local URL: http://localhost:8501
  Network URL: http://192.168.x.x:8501
```

**Acesse**: http://localhost:8501

**Voc√™ deve ver**:
- KPIs agregados
- Tabelas com dados
- Gr√°ficos Plotly
- Dados em tempo real

> **IMPORTANTE**: O dashboard se conecta ao MinIO para ler os dados da camada Gold. Certifique-se de que o MinIO est√° rodando (via Docker ou localmente).

---

## Troubleshooting

### Problema 1: Conda n√£o reconhecido

**Sintoma**: `conda: command not found`

**Solu√ß√£o**:
```bash
# Windows: Adicionar ao PATH manualmente
# Painel de Controle > Sistema > Vari√°veis de Ambiente
# Adicionar: C:\Users\<usuario>\miniconda3\Scripts

# Ou reabrir terminal ap√≥s instala√ß√£o
```

### Problema 2: UV n√£o instala depend√™ncias

**Sintoma**: Erro ao executar `uv sync`

**Solu√ß√£o**:
```bash
# Usar pip tradicional
cd environments
pip install -e .
```

### Problema 3: Docker n√£o inicia

**Sintoma**: Containers ficam em "Exited"

**Solu√ß√£o**:
```bash
# Ver logs
docker-compose logs <container_name>

# Reiniciar Docker Desktop
# Reexecutar
docker-compose down
docker-compose up -d
```

### Problema 4: Python 3.10 n√£o dispon√≠vel

**Sintoma**: `PackageNotFoundError: python=3.10`

**Solu√ß√£o**:
```bash
# Atualizar conda
conda update conda

# Ou usar Python 3.11 (tamb√©m funciona)
conda create -n dataops python=3.11 -y
```

---

## Comandos √öteis

### Gerenciar Ambiente Conda

```bash
# Ativar ambiente
conda activate dataops

# Desativar ambiente
conda deactivate

# Listar ambientes
conda env list

# Remover ambiente
conda env remove -n dataops

# Exportar ambiente
conda env export > environment.yml

# Criar ambiente de um arquivo
conda env create -f environment.yml
```

### Gerenciar Depend√™ncias com UV

```bash
# Adicionar nova depend√™ncia
cd environments
uv add nome_do_pacote

# Remover depend√™ncia
uv remove nome_do_pacote

# Atualizar todas as depend√™ncias
uv sync --upgrade

# Ver depend√™ncias instaladas
uv pip list
```

### Gerenciar Docker

```bash
# Parar containers
docker-compose stop

# Parar e remover containers
docker-compose down

# Ver logs
docker-compose logs -f

# Reiniciar um servi√ßo
docker-compose restart <service_name>

# Rebuild completo
docker-compose build --no-cache
docker-compose up -d
```

---

## Checklist Final

**Instala√ß√£o Base**:
- Conda instalado e funcionando
- Ambiente `dataops` criado com Python 3.10
- UV instalado e funcionando
- Docker + Docker Compose instalados

**Depend√™ncias**:
- Depend√™ncias do Python instaladas (via uv sync)
- Airflow, Pandas, MinIO SDK verificados
- Streamlit funcionando

**Configura√ß√£o**:
- .env criado e preenchido
- Legacy Token do Label Studio obtido
- Containers Docker todos "healthy"

**Funcionalidade**:
- Todos os servi√ßos acess√≠veis
- Pipeline executa sem erros
- Dashboard mostra dados

---

## Automa√ß√£o com Makefile

O projeto inclui um **Makefile** com comandos prontos para facilitar opera√ß√µes comuns.

### Ver Todos os Comandos Dispon√≠veis

```bash
make help
```

### Comandos Mais √öteis

#### Setup e Instala√ß√£o
```bash
# Setup completo (primeira vez)
make first-run

# Criar ambiente conda + instalar depend√™ncias
make setup

# Instalar depend√™ncias (conda j√° ativado)
make install
```

#### Gerenciar Docker
```bash
# Subir containers
make docker-up

# Parar containers
make docker-down

# Reiniciar containers
make docker-restart

# Ver logs
make docker-logs

# Ver status
make docker-status
```

#### Executar Pipeline
```bash
# Pipeline completo (Bronze ‚Üí Silver ‚Üí Gold)
make pipeline

# Limpar buckets
make clean-buckets

# Etapas individuais
make bronze    # Apenas Bronze
make silver    # Apenas Silver
make gold      # Apenas Gold
make diagnose  # Diagn√≥stico
```

#### Dashboard
```bash
# Rodar dashboard localmente
make dashboard
```

#### Desenvolvimento
```bash
# Rodar testes
make test

# Verificar c√≥digo
make lint

# Formatar c√≥digo
make format

# Verificar vari√°veis de ambiente
make check-env
```

### Exemplo de Fluxo Completo

```bash
# 1. Setup inicial (primeira vez)
make first-run

# 2. Ativar ambiente
conda activate dataops

# 3. Configurar .env (manualmente)
notepad .env  # Windows
nano .env     # Linux/Mac

# 4. Executar pipeline
make pipeline

# 5. Ver dashboard
make dashboard
```

---

## Pr√≥ximos Passos

Agora que seu ambiente est√° completo:

1. **Leia a documenta√ß√£o completa**: [SETUP_COMPLETO.md](SETUP_COMPLETO.md)
2. **Execute o pipeline via Airflow** ou scripts locais
3. **Importe o dataset** no Label Studio
4. **Use `make help`** para ver todos os comandos dispon√≠veis

---

## Refer√™ncias

- **Conda**: https://docs.conda.io/
- **UV**: https://github.com/astral-sh/uv
- **Docker**: https://docs.docker.com/
- **Airflow**: https://airflow.apache.org/docs/
- **Label Studio**: https://labelstud.io/guide/

---

**Pronto!** Seu ambiente est√° 100% configurado e pronto para uso!
